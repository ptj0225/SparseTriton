
# %%
import kagglehub

# Download latest version
path = kagglehub.dataset_download("balraj98/modelnet10-princeton-3d-object-dataset")

print("Path to dataset files:", path)

# %%
import torch
import numpy as np
import trimesh
from torch.utils.data import Dataset, DataLoader
from path import Path

def mesh_to_sparse_voxels(path, grid_size=32):
    """trimeshë¥¼ ì‚¬ìš©í•´ ë©”ì‰¬ë¥¼ Voxelë¡œ ë³€í™˜í•˜ê³  ì •ìˆ˜ ì¢Œí‘œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # 1. ë©”ì‰¬ ë¡œë“œ ë° ì •ê·œí™” (ì¤‘ì‹¬ ì´ë™ ë° ìŠ¤ì¼€ì¼ë§)
    mesh = trimesh.load(path)
    
    # 2. Voxelization (pitchëŠ” í•œ ì¹¸ì˜ í¬ê¸°)
    # ì „ì²´ í¬ê¸°(extents)ë¥¼ grid_sizeë¡œ ë‚˜ëˆ„ì–´ í•´ìƒë„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    voxel_grid = mesh.voxelized(pitch=mesh.extents.max() / grid_size)
    
    # 3. ë‚´ë¶€ ì±„ìš°ê¸° (ì„ íƒ ì‚¬í•­: ì†ì´ ë¹ˆ ë©”ì‰¬ë¼ë©´ fill() í˜¸ì¶œ)
    # voxel_grid.fill() 
    
    # 4. ì •ìˆ˜ ì¢Œí‘œ ì¶”ì¶œ (N, 3)
    coords = voxel_grid.sparse_indices.astype(np.int32)
    
    return coords

class SparseModelNetDataset(Dataset):
    def __init__(self, root_dir, folder='train', grid_size=32):
        self.root_dir = Path(root_dir)
        self.grid_size = grid_size
        self.files = []
        
        # ì¹´í…Œê³ ë¦¬ ë¡œë“œ
        self.categories = sorted([d.name for d in self.root_dir.dirs()])
        self.label_map = {name: i for i, name in enumerate(self.categories)}
        
        for cat in self.categories:
            new_dir = self.root_dir / cat / folder
            if new_dir.exists():
                for f in new_dir.files('*.off'):
                    self.files.append({'path': f, 'category': cat})

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_info = self.files[idx]
        
        # trimeshë¥¼ ì´ìš©í•œ voxelize
        try:
            coords = mesh_to_sparse_voxels(file_info['path'], self.grid_size)
        except Exception as e:
            # ê°„í˜¹ ê¹¨ì§„ OFF íŒŒì¼ ëŒ€ì‘: ë‹¨ìˆœ ì •ì  ë¡œë“œ í›„ ì–‘ìí™” (fallback)
            mesh = trimesh.load(file_info['path'])
            pc = mesh.vertices
            pc_normalized = (pc - pc.min(0)) / (pc.max(0) - pc.min(0) + 1e-6)
            coords = np.unique((pc_normalized * (self.grid_size - 1)).astype(np.int32), axis=0)

        # íŠ¹ì§•ê°’ (ê°„ë‹¨íˆ 1.0ìœ¼ë¡œ ì´ˆê¸°í™”)
        feats = np.ones((coords.shape[0], 1), dtype=np.float32)
        label = self.label_map[file_info['category']]
        
        return torch.from_numpy(coords), torch.from_numpy(feats), label
def fast_mesh_to_coords(path, grid_size=32, num_samples=2048 * 4):
    """ë©”ì‰¬ í‘œë©´ì—ì„œ ì ì„ ìƒ˜í”Œë§í•˜ê³  ì¦‰ì‹œ ì •ìˆ˜ ì¢Œí‘œë¡œ ë³€í™˜ (ë§¤ìš° ë¹ ë¦„)"""
    mesh = trimesh.load(path)
    
    # 1. ë©”ì‰¬ í‘œë©´ì—ì„œ ê³ ì •ëœ ê°œìˆ˜ì˜ ì ì„ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ (trimeshì˜ ë¹ ë¥¸ ìƒ˜í”Œë§ ì´ìš©)
    # ë³µì…€í™”ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.
    points = mesh.sample(num_samples)
    
    # 2. ì •ê·œí™” ë° ì–‘ìí™”
    p_min, p_max = points.min(0), points.max(0)
    points = (points - p_min) / (p_max - p_min + 1e-6)
    coords = (points * (grid_size - 1)).astype(np.int32)
    
    # 3. ì¤‘ë³µ ì œê±° (ê°™ì€ ë³µì…€ ì¹¸ì— ë“¤ì–´ê°„ ì ë“¤ í•˜ë‚˜ë¡œ í•©ì¹¨)
    return np.unique(coords, axis=0)

class FastSparseDataset(Dataset):
    def __init__(self, root_dir, folder='train', grid_size=32):
        self.root_dir = Path(root_dir)
        self.grid_size = grid_size
        self.files = []
        
        self.categories = sorted([d.name for d in self.root_dir.dirs()])
        self.label_map = {name: i for i, name in enumerate(self.categories)}
        
        for cat in self.categories:
            new_dir = self.root_dir / cat / folder
            if new_dir.exists():
                self.files.extend([{'path': f, 'label': self.label_map[cat]} for f in new_dir.files('*.off')])

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_info = self.files[idx]
        # ìƒ˜í”Œë§ ë°©ì‹ì„ ì¨ì„œ í›¨ì”¬ ë¹ ë¥´ê²Œ ë¡œë“œ
        coords = fast_mesh_to_coords(file_info['path'], self.grid_size)
        feats = np.ones((coords.shape[0], 1), dtype=np.float32)
        
        return torch.from_numpy(coords), torch.from_numpy(feats), file_info['label']
def sparse_collate_fn(batch):
    coords_list, feats_list, labels = zip(*batch)
    
    batch_coords = []
    for i, coords in enumerate(coords_list):
        # [batch_idx, x, y, z] í˜•íƒœë¡œ ê²°í•©
        b_idx = torch.full((coords.shape[0], 1), i, dtype=torch.int32)
        batch_coords.append(torch.cat([b_idx, coords], dim=1))
    
    return {
        'coords': torch.cat(batch_coords, dim=0),
        'feats': torch.cat(feats_list, dim=0),
        'labels': torch.LongTensor(labels)
    }

# ì‹¤í–‰ ì˜ˆì‹œ
# ds = SparseModelNetDataset(root_dir='./ModelNet10', grid_size=32)
# loader = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=sparse_collate_fn)

# %%
import sparsetriton
from sparsetriton.nn.modules.conv import Conv3d
from sparsetriton.nn.modules.linear import SparseLinear
from sparsetriton.nn.modules.spatial import SparsePooling
from sparsetriton.nn.modules.activation import ReLU
from torch import nn
import torch

from sparsetriton.tensor import SparseTensor

class ResConv3D(nn.Module):
    def __init__(self, in_channels, out_channels, layer_n=3):
        super(ResConv3D, self).__init__()
        self.input_layer = Conv3d(in_channels, out_channels, kernel_size=3)

        self.layers = nn.ModuleList()
        for i in range(layer_n):
            self.layers.append(Conv3d(out_channels, out_channels, kernel_size=3))
            self.layers.append(ReLU())

    def forward(self, x:SparseTensor):
        x_in = self.input_layer(x)
        h = x_in
        for layer in self.layers:
            h = layer(h)    
        y = h.replace(h.F + x_in.F)
        return y


class SparseNet(nn.Module):
    def __init__(self, in_channels, num_classes, layer_n=3):
        super(SparseNet, self).__init__()
        
        self.layers = nn.ModuleList()
        current_channels = in_channels
        base_channels = 32

        for i in range(layer_n):
            # ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì±„ë„ì„ 2ë°°ì”© í™•ì¥ (ìµœëŒ€ 256 ë“± ì œí•œ ê°€ëŠ¥)
            out_channels = base_channels * (2 ** i)
            self.layers.append(ResConv3D(current_channels, out_channels))
            self.layers.append(SparsePooling(kernel_size=3, mode="avg", stride=2, padding=1))
            
            current_channels = out_channels

        self.fc_net = nn.Sequential(
            nn.Linear(current_channels, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x: SparseTensor):

        for layer in self.layers:
            x = layer(x)
        x = x.dense().view(x.batch_size, -1)
        logits = self.fc_net(x)
        return logits

spnet = SparseNet(in_channels=1, num_classes=10, layer_n=6).cuda()

# %%
# ë°ì´í„° ë¡œë“œ (Path ì„¤ì • í•„ìˆ˜)
from sparsetriton.tensor import SparseTensor
import torch.optim as optim
from torch.nn import CrossEntropyLoss
from tqdm import tqdm

ds = FastSparseDataset(root_dir=path + '/ModelNet10', grid_size=64)
loader = DataLoader(ds, batch_size=16, shuffle=True, collate_fn=sparse_collate_fn, num_workers=16)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = spnet.to(device)
criterion = CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
epochs = 20

# DataLoader ì •ì˜ (ìœ„ì—ì„œ ë§Œë“  dsì™€ collate_fn ì‚¬ìš©)
# ds = FastSparseDataset(root_dir=path + '/ModelNet10', grid_size=64)
# loader = DataLoader(ds, batch_size=16, shuffle=True, collate_fn=sparse_collate_fn)

print(f"ğŸš€ Starting training on {device}...")

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    # 2. tqdm ì§„í–‰ í‘œì‹œì¤„ ì„¤ì •
    pbar = tqdm(enumerate(loader), total=len(loader), desc=f"Epoch {epoch+1}/{epochs}")
    
    for i, batch in pbar:
        # ë°ì´í„° ì¤€ë¹„
        coords = batch['coords']
        feats = batch['feats']
        labels = batch['labels'].to(device)
        
        # SparseTensor ìƒì„±
        input_tensor = SparseTensor(feats, coords).to(device)
        # Forward pass
        optimizer.zero_grad()
        outputs = model(input_tensor)
        
        loss = criterion(outputs, labels)
        
        # Backward & Optimize
        loss.backward()
        
        optimizer.step()
        
        # í†µê³„ ê³„ì‚°
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # 3. tqdm ì„¤ëª…ì¹¸ì— ì‹¤ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
        current_acc = 100. * correct / total
        pbar.set_postfix({
            'loss': f"{loss.item():.4f}", 
            'acc': f"{current_acc:.2f}%"
        })

    # ì—í­ ì¢…ë£Œ í›„ ìš”ì•½ ì¶œë ¥
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    print(f"âœ… Epoch {epoch+1} Results -> Avg Loss: {epoch_loss:.4f}, Avg Acc: {epoch_acc:.2f}%")

# 4. í…ŒìŠ¤íŠ¸ ë£¨í”„ (ë§ˆì§€ë§‰ì— í•œ ë²ˆ)
print("\nğŸ” Running Final Test...")
test_ds = FastSparseDataset(root_dir=path + '/ModelNet10', folder='test', grid_size=64)
test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, collate_fn=sparse_collate_fn)

model.eval()
test_correct = 0
test_total = 0

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Testing"):
        input_tensor = SparseTensor(batch['feats'], batch['coords'], batch_size=len(labels), spatial_shape=(64, 64, 64)).to(device)
        outputs = model(input_tensor)
        
        if outputs.dim() > 2:
            outputs = outputs.max(dim=1)[0].max(dim=1)[0].max(dim=1)[0]
            
        _, predicted = outputs.max(1)
        test_total += batch['labels'].size(0)
        test_correct += predicted.eq(batch['labels'].to(device)).sum().item()

print(f"\nğŸ† Final Test Accuracy: {100. * test_correct / test_total:.2f}%")

# %%
input_tensor.F.max()

# %%
loss

# %%
outputs

# %%
for name, param in model.named_parameters():
    if param.grad is not None:
        norm = param.grad.norm(2).item()
        print(f"Layer: {name:<30} | Grad Norm: {norm:.6f}")
print("="*50 + "\n")

# ìµœì í™” ë‹¨ê³„
optimizer.step()
optimizer.zero_grad()

# %%
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()

# %%
print(f"âœ… Epoch {epoch+1} Results -> Avg Loss: {epoch_loss:.4f}, Avg Acc: {epoch_acc:.2f}%")

# %%
import sparsetriton
from sparsetriton.nn.modules.conv import Conv3d
from sparsetriton.nn.modules.linear import SparseLinear
from sparsetriton.nn.modules.spatial import SparsePooling
from sparsetriton.nn.modules.activation import ReLU
from torch import nn
import torch

from sparsetriton.tensor import SparseTensor

class ResConv3D(nn.Module):
    def __init__(self, in_channels, out_channels, layer_n=3):
        super(ResConv3D, self).__init__()
        self.input_layer = Conv3d(in_channels, out_channels, kernel_size=3)

        self.layers = nn.ModuleList()
        for i in range(layer_n):
            self.layers.append(Conv3d(out_channels, out_channels, kernel_size=3))
            self.layers.append(ReLU())

    def forward(self, x:SparseTensor):
        x_in = self.input_layer(x)
        h = x_in
        for layer in self.layers:
            h = layer(h)    
        y = h.replace(h.F + x_in.F)
        return y


class SparseNet(nn.Module):
    def __init__(self, in_channels, num_classes, layer_n=3):
        super(SparseNet, self).__init__()
        
        self.layers = nn.ModuleList()
        current_channels = in_channels
        base_channels = 32

        for i in range(layer_n):
            # ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì±„ë„ì„ 2ë°°ì”© í™•ì¥ (ìµœëŒ€ 256 ë“± ì œí•œ ê°€ëŠ¥)
            out_channels = base_channels * (2 ** i)
            self.layers.append(ResConv3D(current_channels, out_channels))
            self.layers.append(SparsePooling(kernel_size=3, mode="avg", stride=2, padding=1))
            
            current_channels = out_channels

        self.fc_net = nn.Sequential(
            nn.Linear(current_channels, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x: SparseTensor):

        print(x.F.abs().sum())
        for layer in self.layers:
            x = layer(x)
            print(x.F.abs().sum())
        
        x = x.dense().view(x.batch_size, -1)
        logits = self.fc_net(x)
        return logits

spnet = SparseNet(in_channels=1, num_classes=10, layer_n=6).cuda()

outputs = spnet(input_tensor)
outputs

# %%
outputs

# %%
from torch.nn import AvgPool3d
import torch

x = torch.rand((1, 3, 15, 15, 15), device="cuda", dtype=torch.float32)
pool = AvgPool3d(1, 2, padding=0).cuda()
pool(x).shape

# %%
pool = SparsePooling(3, stride=2, padding=1, mode="avg").cuda()

# %%
input_tensor.F.requires_grad = True

# %%
pool(input_tensor).F.sum().backward()

# %%
input_tensor.F.grad.max()

# %%
30 // 2

# %%
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import torch # torchê°€ ì„í¬íŠ¸ë˜ì–´ ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€

# ì‹œê°í™”ë¥¼ ìœ„í•´ ë£¨í”„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ê³  ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ë©ˆì¶¥ë‹ˆë‹¤.
# dsì™€ loader ë³€ìˆ˜ê°€ ì´ì „ì— ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ë§Œì•½ ds ë˜ëŠ” loaderê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
try:
    _ = loader # loaderê°€ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
except NameError:
    print("Error: 'loader' is not defined. Please run the preceding cells first.")
    # ì£¼ì˜: ì´ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë³´ì´ë©´, ë°ì´í„°ì…‹ê³¼ ë¡œë”ë¥¼ ì •ì˜í•˜ëŠ” ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆë¥¼ ë“¤ì–´, path ë³€ìˆ˜ì™€ SparseModelNetDataset, DataLoader ì •ì˜ ì…€ì„ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.


for batch in loader:
    coords = batch['coords']
    labels = batch['labels']

    # ì²« ë²ˆì§¸ ì•„ì´í…œì˜ ì¢Œí‘œë§Œ ì„ íƒ (batch_idx == 0)
    # item_coordsëŠ” í…ì„œì´ë©°, .cpu().numpy()ë¥¼ ì‚¬ìš©í•˜ì—¬ matplotlibì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    item_coords = coords[coords[:, 0] == 0][:, 1:] # x, y, z ì¢Œí‘œ
    item_label_id = labels[0].item()

    # í´ë˜ìŠ¤ ì´ë¦„ ì°¾ê¸°
    class_name = [name for name, idx in ds.label_map.items() if idx == item_label_id][0]

    # 3D ì‹œê°í™”
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111, projection='3d')
    # matplotlibëŠ” numpy ë°°ì—´ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ .cpu().numpy()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ax.scatter(item_coords[:, 0].cpu().numpy(), item_coords[:, 1].cpu().numpy(), item_coords[:, 2].cpu().numpy(), s=15) # sëŠ” ì ì˜ í¬ê¸°

    ax.set_title(f'Category: {class_name} (Label ID: {item_label_id})')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')

    # ì¶•ì˜ ìŠ¤ì¼€ì¼ì„ ë™ì¼í•˜ê²Œ ë§ì¶”ì–´ ì™œê³¡ ë°©ì§€
    # item_coordsê°€ CUDA í…ì„œì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ .cpu() í˜¸ì¶œ ì¶”ê°€
    max_range = (item_coords.max(dim=0)[0] - item_coords.min(dim=0)[0]).max().item()
    mid = item_coords.float().mean(dim=0) # í‰ê·  ê³„ì‚° ì‹œ floatë¡œ ë³€í™˜í•˜ì—¬ ì •í™•ì„± ìœ ì§€
    ax.set_xlim(mid[0].item() - max_range / 2, mid[0].item() + max_range / 2)
    ax.set_ylim(mid[1].item() - max_range / 2, mid[1].item() + max_range / 2)
    ax.set_zlim(mid[2].item() - max_range / 2, mid[2].item() + max_range / 2)

    plt.show()

    break # í•˜ë‚˜ì˜ ë°°ì¹˜ë§Œ ì‹œê°í™”í•˜ê³  ë©ˆì¶¤


